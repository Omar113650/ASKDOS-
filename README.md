# ğŸ“„ Smart Contract RAG Assistant

A Retrieval-Augmented Generation (RAG) system that allows users to upload a contract (PDF or DOCX) and ask questions about it using a conversational interface with memory.

Built with **FastAPI**, **LangChain (LCEL)**, **ChromaDB**, **Ollama (Mistral)**, and **Gradio**.

---

## ğŸ¥ Demo

Below is a short demo of the system in action:

[ğŸ¬ Watch Demo](assets/demo.mp4)

> If the video does not preview directly on GitHub, download it from the link above.


## ğŸš€ Features

- ğŸ“‚ Upload contract documents (PDF / DOCX)
- ğŸ” Automatic document chunking & embeddings
- ğŸ§  Vector search using ChromaDB
- ğŸ¤– RAG pipeline powered by Mistral (via Ollama)
- ğŸ’¬ Conversational memory (chat history support)
- ğŸŒ REST API with FastAPI + LangServe
- ğŸ–¥ Modern Chat UI using Gradio

---

## ğŸ— Architecture Overview

1. **Document Ingestion**
   - Load PDF/DOCX
   - Split into chunks
   - Generate embeddings (HuggingFace)
   - Store vectors in ChromaDB

2. **RAG Pipeline**
   - Retrieve top-k relevant chunks
   - Inject context into prompt
   - Generate answer using Mistral (Ollama)

3. **Conversation Memory**
   - Managed via `RunnableWithMessageHistory`
   - Session-based chat history

4. **Frontend**
   - Upload document
   - Chat with memory-enabled assistant

---

## ğŸ›  Tech Stack

- Python
- FastAPI
- LangChain (LCEL)
- ChromaDB
- Ollama (Mistral model)
- HuggingFace Embeddings
- Gradio (v6+)

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Youssef-Osama1/smart-contract-assistant.git
cd smart-contract-assistant
```
### 2ï¸âƒ£ Create environment

```bash
conda create -n smart_assistant python=3.11 -y
conda activate smart_assistant
```
### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```
---

## âš™ï¸ Setup Ollama

Install Ollama from:

https://ollama.com

Then pull the Mistral model:
```bash
ollama pull mistral
```
Make sure Ollama is running locally.

---

## â–¶ï¸ Run the Backend
```bash
uvicorn backend.main:app --reload
```
Backend runs at:

http://localhost:8000

---

## â–¶ï¸ Run the Frontend
```bash
python frontend/app.py
```
Frontend runs at:

http://127.0.0.1:7860

---

## ğŸ’¬ How It Works

1. Upload a contract
2. The system builds a vector store
3. Ask questions about the contract
4. The assistant:
   - Retrieves relevant context
   - Uses memory of previous messages
   - Generates accurate responses

---

## ğŸ“‚ Project Structure
```
smart-contract-assistant/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”œâ”€â”€ rag_chain.py
â”‚   â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
|
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ demo.mp4
|
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Example Use Cases

- Contract review
- Clause explanation
- Obligation clarification
- Risk identification
- Legal Q&A assistance

---

## ğŸ”® Future Improvements

- Persistent memory storage
- User-based session IDs
- Streaming responses
- Authentication
- Docker deployment
- Cloud deployment (AWS / Azure)
